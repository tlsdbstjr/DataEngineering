{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "945c0de3-d079-4a39-8df4-87a60678b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b04224",
   "metadata": {},
   "source": [
    "# ETL Process\n",
    "다음은 wikipedia에서 표를 읽고 json파일로 저장하는 ETL 프로세스 입니다. 로그 파일도 같이 작성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f97f603b-0954-4c80-9d51-9a3ef1c8283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu, 09 Jan 2025 08:54:04 GMT\n",
      "304\n",
      "Thu, 09 Jan 2025 08:54:04 GMT\n",
      "304\n",
      "0     United States\n",
      "1             China\n",
      "2           Germany\n",
      "3             Japan\n",
      "4             India\n",
      "          ...      \n",
      "68            Kenya\n",
      "69           Angola\n",
      "70        Guatemala\n",
      "71             Oman\n",
      "72        Venezuela\n",
      "Name: Country/Territory, Length: 72, dtype: object\n",
      "Geographical subregion\n",
      "Australia and New Zealand    1072.030000\n",
      "Caribbean                       0.142000\n",
      "Central America                36.412000\n",
      "Central Asia                  108.958000\n",
      "Eastern Africa                  2.212000\n",
      "Eastern Asia                 7982.053333\n",
      "Eastern Europe                168.324000\n",
      "Melanesia                       8.104000\n",
      "Micronesia                      0.270000\n",
      "Middle Africa                  21.324000\n",
      "Northern Africa               112.238000\n",
      "Northern America              776.770000\n",
      "Northern Europe               104.852000\n",
      "Polynesia                       0.417500\n",
      "South America                  54.898000\n",
      "South-eastern Asia            423.600000\n",
      "Southern Africa               113.660000\n",
      "Southern Asia                  28.660000\n",
      "Southern Europe                10.510000\n",
      "Western Africa                  4.796000\n",
      "Western Asia                   26.422000\n",
      "Western Europe                267.958000\n",
      "Name: IMF Forecast, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/gj1qtfn50f91xp32471rcq500000gn/T/ipykernel_82249/3808746885.py:132: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top5 = gdpData.groupby('Geographical subregion').apply(lambda x: x.sort_values('IMF Forecast').head(5))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "LOG_DIR = \"./etl_project_log.txt\"\n",
    "\n",
    "# urls\n",
    "urlGDP = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29'\n",
    "urlRegion = 'https://en.wikipedia.org/wiki/List_of_countries_and_territories_by_the_United_Nations_geoscheme'\n",
    "\n",
    "# backup file name\n",
    "bakupFile = {urlGDP:\"wikipediaGDP\", urlRegion:\"wikipediaRegion\"}\n",
    "\n",
    "# decorator for logging\n",
    "# this decorator helps to log the ETL processes.\n",
    "# function name, its start and end time, and running time will be logged.\n",
    "def withLog(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with open(LOG_DIR, \"a\") as f:\n",
    "            f.write(datetime.datetime.now().strftime(\"%Y-%B-%d-%H-%M-%S,\"))\n",
    "            f.write(f\"{func.__name__},start\\n\")\n",
    "            startTime = datetime.datetime.now()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            endTime = datetime.datetime.now()\n",
    "            f.write(endTime.strftime(\"%Y-%B-%d-%H-%M-%S,\"))\n",
    "            f.write(f\"{func.__name__},end,{endTime-startTime}{\",\"+args[0] if func.__name__ == \"extract\" else \"\"}\\n\")\n",
    "        return result\n",
    "    return wrapper   \n",
    "\n",
    "\n",
    "# get the gdp data from wikipedia\n",
    "@withLog\n",
    "def extract(url):\n",
    "    # find past extract log\n",
    "    logs = pd.read_csv(LOG_DIR, header=None, names=[\"time\", \"function\", \"status\" ,\"taken\", \"url\"])\n",
    "    lastAccess = logs[logs[\"url\"] == url]\n",
    "    \n",
    "    # get http responce\n",
    "    if lastAccess.empty:\n",
    "        response = requests.get(url)\n",
    "    else:\n",
    "        lastAccess = datetime.datetime.strptime(lastAccess.iloc[-1]['time'],\"%Y-%B-%d-%H-%M-%S\") - datetime.timedelta(hours=9)\n",
    "        response = requests.get(url, headers={\"if-Modified-Since\":lastAccess.strftime('%a, %d %b %Y %H:%M:%S GMT')})\n",
    "    \n",
    "    table = \"\"\n",
    "\n",
    "    # check reponse\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        table = str(soup.select(\"table.wikitable\")[0])\n",
    "        \n",
    "        # save the responce\n",
    "        with open(bakupFile[url]+\".bak\", \"w\") as f:\n",
    "            f.write(str(table))\n",
    "\n",
    "    elif response.status_code == 304:\n",
    "        # open from backup file\n",
    "        try:\n",
    "            with open(bakupFile[url]+\".bak\", \"r\") as f:\n",
    "                table = f.read()\n",
    "        except:\n",
    "            print(response.status_code)        \n",
    "    else : \n",
    "        print(response.status_code)\n",
    "\n",
    "    return table\n",
    "    \n",
    "\n",
    "# transform the html table data to pandas data frame and process the data\n",
    "@withLog\n",
    "def transform(data):\n",
    "    # html table to pandas data frame\n",
    "    df = pd.read_html(StringIO(data))[0]\n",
    "\n",
    "    # delete annotation\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: re.sub(r'\\[.*?\\]', '', str(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "@withLog\n",
    "def transformGDPTable(df):\n",
    "    df = df.droplevel(level=0, axis=1)  # drop level. because its data has multy-level columns\n",
    "    df.columns = ['Country/Territory', 'IMF Forecast', 'IMF Year', 'World Bank Estimate', 'World BankYear', 'UN Estimate',\n",
    "       'UN Year']\n",
    "    cols = df.columns\n",
    "\n",
    "    for col in cols[2::2]:  # for year columns, transform the data to integer\n",
    "        df[col] = df[col].apply(lambda x: int(x) if x.isdigit() else -1)\n",
    "    for col in cols[1::2]:  # for GDP data, make billion unit and float\n",
    "        df[col] = df[col].apply(lambda x: round(float(x)/1000.0, 2) if x.isdigit() else 0.0)\n",
    "\n",
    "    df = df.drop(index = [0])\n",
    "            \n",
    "    return df\n",
    "\n",
    "@withLog\n",
    "def transformConjungrate(gdp, region):\n",
    "    #_gdp = gdp.reset_index(drop=True)\n",
    "    #_region = region.reset_index(drop=True)\n",
    "\n",
    "    res = gdp.merge(\n",
    "        region[['Country or Area', 'Geographical subregion']],\n",
    "        left_on='Country/Territory',\n",
    "        right_on='Country or Area',\n",
    "        how='left'\n",
    "    )\n",
    "    res = res.drop('Country or Area', axis=1)\n",
    "\n",
    "    return res\n",
    "\n",
    "# write the dataFrame to a json file\n",
    "@withLog\n",
    "def load(dataFrame):\n",
    "    # extract dataFrame by json\n",
    "    dataFrame.transpose().to_json('Countries_by_GDP.json')\n",
    "\n",
    "def printOver100B(gdpData):\n",
    "    print(gdpData[gdpData['IMF Forecast'] >= 100.0]['Country/Territory'])\n",
    "\n",
    "def printTop5(gdpData):\n",
    "    top5 = gdpData.groupby('Geographical subregion').apply(lambda x: x.sort_values('IMF Forecast').head(5))\n",
    "    top5 = top5.reset_index(level = 0, drop=True)\n",
    "    top5 = top5.groupby('Geographical subregion')['IMF Forecast'].mean()\n",
    "    print(top5)\n",
    "\n",
    "# main\n",
    "\n",
    "# gdp ETL process\n",
    "gdp = extract(urlGDP)\n",
    "gdp = transform(gdp)\n",
    "gdp = transformGDPTable(gdp)\n",
    "\n",
    "# region ETL process\n",
    "region = extract(urlRegion)\n",
    "region = transform(region)\n",
    "\n",
    "# merge two tables\n",
    "gdpTable = transformConjungrate(gdp, region)\n",
    "\n",
    "# and load merged table\n",
    "load(gdpTable)\n",
    "\n",
    "# print what we want\n",
    "printOver100B(gdpTable)\n",
    "printTop5(gdpTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84fa1c",
   "metadata": {},
   "source": [
    "# ETL Process with IMF API\n",
    "다음은 IMF에서 GDP정보를 직접 받아와서 작업을 처리하는 ETL입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b723395-f6da-4690-9386-19691638befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                Algeria\n",
      "2                                                 Angola\n",
      "4                                              Argentina\n",
      "5                                              Australia\n",
      "6                                                Austria\n",
      "                             ...                        \n",
      "131                                              Türkiye\n",
      "133                                 United Arab Emirates\n",
      "134    United Kingdom of Great Britain and Northern I...\n",
      "135                             United States of America\n",
      "138                                             Viet Nam\n",
      "Name: name, Length: 61, dtype: object\n",
      "sub-region\n",
      "Australia and New Zealand           1027.1210\n",
      "Eastern Asia                        4927.7574\n",
      "Eastern Europe                       395.1750\n",
      "Latin America and the Caribbean        1.3438\n",
      "Melanesia                             10.1420\n",
      "Micronesia                             0.3120\n",
      "Northern Africa                      108.8924\n",
      "Northern America                   15691.2875\n",
      "Northern Europe                      363.1226\n",
      "Polynesia                              0.8065\n",
      "South-eastern Asia                   384.4326\n",
      "Southern Asia                        187.9038\n",
      "Southern Europe                      467.5520\n",
      "Sub-Saharan Africa                     1.7782\n",
      "Western Asia                          93.4612\n",
      "Western Europe                       689.9726\n",
      "Name: 2024, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/gj1qtfn50f91xp32471rcq500000gn/T/ipykernel_76902/1155763041.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top5 = gdpData.groupby('sub-region').apply(lambda x: x.sort_values(str(year)).head(5))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# decorator for logging\n",
    "# this decorator helps to log the ETL processes.\n",
    "# function name, its start and end time, and running time will be logged.\n",
    "def withLog(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with open(\"./etl_project_log.txt\", \"a\") as f:\n",
    "            f.write(datetime.datetime.now().strftime(\"%Y-%B-%d-%H-%M-%S,\"))\n",
    "            f.write(f\"{func.__name__},start\\n\")\n",
    "            startTime = datetime.datetime.now()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            endTime = datetime.datetime.now()\n",
    "            f.write(endTime.strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__},end,{endTime-startTime}\\n\")\n",
    "        return result\n",
    "    return wrapper   \n",
    "\n",
    "# get the gdp data from wikipedia\n",
    "#@withLog\n",
    "def extract_IMF_GDP(url):\n",
    "    # get http response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check reponse\n",
    "    if response.status_code == 200:\n",
    "        GDPJson= response.text\n",
    "    else : \n",
    "        print(response.status_code)\n",
    "\n",
    "    return GDPJson\n",
    "\n",
    "# transform the html table data to pandas data frame and process the data\n",
    "#@withLog\n",
    "def transform_IMF_GDP(data):\n",
    "    # html table to pandas data frame\n",
    "    df = json.loads(data)\n",
    "    df = df['values']['NGDPD']\n",
    "    df = pd.DataFrame(df).transpose()\n",
    "    #df = df.fillna(0.0)\n",
    "    df = df.dropna()    \n",
    "    return df\n",
    "\n",
    "# merge  imfData with ISO_Countries format data\n",
    "def mergeCountries(imfData, ISO_Countries):\n",
    "    iso = ISO_Countries.drop(columns=['alpha-2', 'country-code', 'iso_3166-2', 'region-code', 'sub-region-code', 'intermediate-region-code'])\n",
    "    imfData = imfData.reset_index()\n",
    "    result = pd.merge(\n",
    "        left=iso,\n",
    "        right=imfData,\n",
    "        left_on='alpha-3',\n",
    "        right_on='index',\n",
    "        how='inner'\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def printOver100B(gdpData, year):\n",
    "    print(gdpData[gdpData[str(year)] >= 100.0]['name'])\n",
    "\n",
    "# prin top 5 countries' gdp data average, year is integer and selectable\n",
    "def printTop5(gdpData, year):\n",
    "    top5 = gdpData.groupby('sub-region').apply(lambda x: x.sort_values(str(year)).head(5))\n",
    "    top5 = top5.reset_index(level = 0, drop=True)\n",
    "    top5 = top5.groupby('sub-region')[str(year)].mean()\n",
    "    print(top5)\n",
    "\n",
    "# write the dataFrame to a json file\n",
    "#@withLog\n",
    "def load(dataFrame):\n",
    "    # extract dataFrame by json\n",
    "    dataFrame.transpose().to_json('Countries_by_GDP.json')        \n",
    "\n",
    "ISO_Countries = pd.read_csv('./ISO_3166_Countries.csv')\n",
    "url = 'https://www.imf.org/external/datamapper/api/v1/NGDPD'\n",
    "\n",
    "a = transform_IMF_GDP(extract_IMF_GDP(url))\n",
    "b = mergeCountries(a, ISO_Countries)\n",
    "\n",
    "printOver100B(b, 2024)\n",
    "printTop5(b, 2024)\n",
    "\n",
    "load(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc1146",
   "metadata": {},
   "source": [
    "# ETL Process with sql\n",
    "다음은 IMF 에서 받은 데이터를 SQL에 load하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "# data base settings\n",
    "database = \"World_Economies.db\"\n",
    "\n",
    "# function to send a query\n",
    "def sendQuery(sql):\n",
    "    try:\n",
    "        with sqlite3.connect(database) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql)\n",
    "            conn.commit()\n",
    "            return cursor.fetchall()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    "\n",
    "LOG_DIR = \"./etl_project_log.txt\"\n",
    "\n",
    "# urls\n",
    "url = 'https://www.imf.org/external/datamapper/api/v1/NGDPD'\n",
    "\n",
    "# backup file name\n",
    "bakupFile = {url:\"imfGDP\"}\n",
    "\n",
    "# decorator for logging\n",
    "# this decorator helps to log the ETL processes.\n",
    "# function name, its start and end time, and running time will be logged.\n",
    "def withLog(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with open(LOG_DIR, \"a\") as f:\n",
    "            f.write(datetime.datetime.now().strftime(\"%Y-%B-%d-%H-%M-%S,\"))\n",
    "            f.write(f\"{func.__name__},start\\n\")\n",
    "            startTime = datetime.datetime.now()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            endTime = datetime.datetime.now()\n",
    "            f.write(endTime.strftime(\"%Y-%B-%d-%H-%M-%S,\"))\n",
    "            f.write(f\"{func.__name__},end,{endTime-startTime}{\",\"+args[0] if func.__name__ == \"extract\" else \"\"}\\n\")\n",
    "        return result\n",
    "    return wrapper   \n",
    "\n",
    "# get the gdp data from wikipedia\n",
    "@withLog\n",
    "def extract(url):\n",
    "    # find past extract log\n",
    "    logs = pd.read_csv(LOG_DIR, header=None, names=[\"time\", \"function\", \"status\" ,\"taken\", \"url\"])\n",
    "    lastAccess = logs[logs[\"url\"] == url]\n",
    "    \n",
    "    # get response\n",
    "    if lastAccess.empty:\n",
    "        response = requests.get(url)\n",
    "    else:\n",
    "        lastAccess = datetime.datetime.strptime(lastAccess.iloc[-1]['time'],\"%Y-%B-%d-%H-%M-%S\") - datetime.timedelta(hours=9)\n",
    "        response = requests.get(url, headers={\"if-Modified-Since\":lastAccess.strftime('%a, %d %b %Y %H:%M:%S GMT')})\n",
    "    \n",
    "    GDPJson = \"\"\n",
    "\n",
    "    # check reponse\n",
    "    if response.status_code == 200:\n",
    "        GDPJson = response.text\n",
    "        \n",
    "        # save the response\n",
    "        with open(bakupFile[url]+\".bak\", \"w\") as f:\n",
    "            f.write(GDPJson)\n",
    "\n",
    "    elif response.status_code == 304:\n",
    "        # read saved response\n",
    "        try:\n",
    "            with open(bakupFile[url]+\".bak\", \"r\") as f:\n",
    "                GDPJson = f.read()\n",
    "        except:\n",
    "            print(response.status_code) \n",
    "    else : \n",
    "        print(response.status_code)\n",
    "\n",
    "    return GDPJson\n",
    "\n",
    "# transform the html table data to pandas data frame and process the data\n",
    "@withLog\n",
    "def transform_IMF_GDP(data):\n",
    "    # html table to pandas data frame\n",
    "    df = json.loads(data)\n",
    "    df = df['values']['NGDPD']\n",
    "    df = pd.DataFrame(df).transpose()\n",
    "    return df\n",
    "\n",
    "# write the dataFrame to a json file\n",
    "@withLog\n",
    "def load(dataFrame:pd.DataFrame, tableName:str, index:bool = False, index_label:str = \"\"):\n",
    "    # extract dataFrame by jsontry:\n",
    "    try:\n",
    "        with sqlite3.connect(database) as conn:\n",
    "            # Add table name 'gdp_data' and if_exists parameter\n",
    "            dataFrame.to_sql(tableName, conn, if_exists='replace', index=index, index_label=\"Country\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    \n",
    "# main\n",
    "\n",
    "# import gdp data from imf\n",
    "imf_gdp = transform_IMF_GDP(extract(url))\n",
    "load(imf_gdp, \"imf_gdp\", index=True, index_label=\"country\")\n",
    "\n",
    "# import iso countries' name data from saved csv file\n",
    "ISO_Countries = pd.read_csv('./ISO_3166_Countries.csv')\n",
    "load(ISO_Countries, \"iso_country_name\")\n",
    "\n",
    "year = 2024\n",
    "\n",
    "# print countries whose GDP is over 100B\n",
    "sql = f\"\"\"SELECT DISTINCT i.name\n",
    "         FROM imf_gdp g\n",
    "         JOIN iso_country_name i ON i.\"alpha-3\" = g.country\n",
    "         WHERE g.\"{year}\" >= 100.0;\"\"\"\n",
    "print(*sendQuery(sql), sep=\"\\n\")\n",
    "\n",
    "# print top 5 GDP mean of group by region\n",
    "sql =f\"\"\"WITH ranked_countries AS (\n",
    "            SELECT \n",
    "                g.country,\n",
    "                i.\"sub-region\",\n",
    "                g.\"{year}\" as gdp,\n",
    "                RANK() OVER (PARTITION BY i.\"sub-region\" ORDER BY g.\"{year}\" DESC) as rank\n",
    "            FROM imf_gdp g\n",
    "            JOIN iso_country_name i ON i.\"alpha-3\" = g.country\n",
    "            WHERE g.\"{year}\" IS NOT NULL\n",
    "        )\n",
    "        SELECT \n",
    "            \"sub-region\",\n",
    "            COUNT(country) as country_count,\n",
    "            ROUND(AVG(gdp), 2) as avg_gdp\n",
    "        FROM ranked_countries\n",
    "        WHERE rank <= 5\n",
    "        GROUP BY \"sub-region\"\n",
    "        ORDER BY avg_gdp DESC;\"\"\"\n",
    "print(*sendQuery(sql), sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
