{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "945c0de3-d079-4a39-8df4-87a60678b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b04224",
   "metadata": {},
   "source": [
    "# ETL Process\n",
    "다음은 wikipedia에서 표를 읽고 json파일로 저장하는 ETL 프로세스 입니다. 로그 파일도 같이 작성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f603b-0954-4c80-9d51-9a3ef1c8283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# decorator for logging\n",
    "# this decorator helps to log the ETL processes.\n",
    "# function name, its start and end time, and running time will be logged.\n",
    "def withLog(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with open(\"./etl_project_log.txt\", \"a\") as f:\n",
    "            f.write(datetime.datetime.now().strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__} start\\n\")\n",
    "            startTime = datetime.datetime.now()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            endTime = datetime.datetime.now()\n",
    "            f.write(endTime.strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__} end. in {endTime-startTime}\\n\")\n",
    "        return result\n",
    "    return wrapper   \n",
    "\n",
    "\n",
    "# get the gdp data from wikipedia\n",
    "@withLog\n",
    "def extract(url):\n",
    "    # get http response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check reponse\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    else : \n",
    "        print(response.status_code)\n",
    "\n",
    "    # find all tables\n",
    "    table = soup.select(\"table.wikitable\")\n",
    "\n",
    "    return table\n",
    "\n",
    "# transform the html table data to pandas data frame and process the data\n",
    "@withLog\n",
    "def transform(data):\n",
    "    # html table to pandas data frame\n",
    "    df = pd.read_html(StringIO(str(data)))[0]\n",
    "\n",
    "    # delete annotation\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: re.sub(r'\\[.*?\\]', '', str(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "@withLog\n",
    "def transformGDPTable(df):\n",
    "    df = df.droplevel(level=0, axis=1)\n",
    "    df.columns = ['Country/Territory', 'IMF Forecast', 'IMF Year', 'World Bank Estimate', 'World BankYear', 'UN Estimate',\n",
    "       'UN Year']\n",
    "    cols = df.columns\n",
    "\n",
    "    for col in cols[2::2]:  # for year columns, transform the data to integer\n",
    "        df[col] = df[col].apply(lambda x: int(x) if x.isdigit() else -1)\n",
    "    for col in cols[1::2]:  # for GDP data, make billion unit and float\n",
    "        df[col] = df[col].apply(lambda x: round(float(x)/1000.0, 2) if x.isdigit() else 0.0)\n",
    "\n",
    "    df = df.drop(index = [0])\n",
    "            \n",
    "    return df\n",
    "\n",
    "@withLog\n",
    "def transformConjungrate(gdp, region):\n",
    "    #_gdp = gdp.reset_index(drop=True)\n",
    "    #_region = region.reset_index(drop=True)\n",
    "\n",
    "    res = gdp.merge(\n",
    "        region[['Country or Area', 'Geographical subregion']],\n",
    "        left_on='Country/Territory',\n",
    "        right_on='Country or Area',\n",
    "        how='left'\n",
    "    )\n",
    "    res = res.drop('Country or Area', axis=1)\n",
    "\n",
    "    return res\n",
    "\n",
    "# write the dataFrame to a json file\n",
    "@withLog\n",
    "def load(dataFrame):\n",
    "    # extract dataFrame by json\n",
    "    dataFrame.transpose().to_json('Countries_by_GDP.json')\n",
    "\n",
    "def printOver100B(gdpData):\n",
    "    print(gdpData[gdpData['IMF Forecast'] >= 100.0]['Country/Territory'])\n",
    "\n",
    "def printTop5(gdpData):\n",
    "    top5 = gdpData.groupby('Geographical subregion').apply(lambda x: x.sort_values('IMF Forecast').head(5))\n",
    "    top5 = top5.reset_index(level = 0, drop=True)\n",
    "    top5 = top5.groupby('Geographical subregion')['IMF Forecast'].mean()\n",
    "    print(top5)\n",
    "\n",
    "# main\n",
    "# urls\n",
    "urlGDP = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29'\n",
    "urlRegion = 'https://en.wikipedia.org/wiki/List_of_countries_and_territories_by_the_United_Nations_geoscheme'\n",
    "\n",
    "# gdp ETL process\n",
    "gdp = extract(urlGDP)\n",
    "gdp = transform(gdp)\n",
    "gdp = transformGDPTable(gdp)\n",
    "\n",
    "# region ETL process\n",
    "region = extract(urlRegion)\n",
    "region = transform(region)\n",
    "\n",
    "# merge two tables\n",
    "gdpTable = transformConjungrate(gdp, region)\n",
    "\n",
    "# print what we want\n",
    "printOver100B(gdpTable)\n",
    "printTop5(gdpTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84fa1c",
   "metadata": {},
   "source": [
    "# ETL Process with IMF API\n",
    "다음은 IMF에서 GDP정보를 직접 받아와서 작업을 처리하는 ETL입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9b723395-f6da-4690-9386-19691638befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2                                                Algeria\n",
      "4                                                 Angola\n",
      "6                                              Argentina\n",
      "9                                              Australia\n",
      "10                                               Austria\n",
      "                             ...                        \n",
      "184    United Kingdom of Great Britain and Northern I...\n",
      "185                             United States of America\n",
      "187                                           Uzbekistan\n",
      "189                    Venezuela, Bolivarian Republic of\n",
      "190                                             Viet Nam\n",
      "Name: name, Length: 72, dtype: object\n",
      "sub-region\n",
      "Australia and New Zealand           1027.1210\n",
      "Central Asia                         103.5716\n",
      "Eastern Asia                        1283.7754\n",
      "Eastern Europe                       105.2664\n",
      "Latin America and the Caribbean        1.3438\n",
      "Melanesia                             10.1420\n",
      "Micronesia                             0.3102\n",
      "Northern Africa                      108.8924\n",
      "Northern America                   15691.2875\n",
      "Northern Europe                      102.0712\n",
      "Polynesia                              0.5620\n",
      "South-eastern Asia                    28.8164\n",
      "Southern Asia                         10.7616\n",
      "Southern Europe                       10.8694\n",
      "Sub-Saharan Africa                     1.3188\n",
      "Western Asia                          14.9268\n",
      "Western Europe                       689.9726\n",
      "Name: 2024, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/gj1qtfn50f91xp32471rcq500000gn/T/ipykernel_72626/2962036843.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top5 = gdpData.groupby('sub-region').apply(lambda x: x.sort_values(str(year)).head(5))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# decorator for logging\n",
    "# this decorator helps to log the ETL processes.\n",
    "# function name, its start and end time, and running time will be logged.\n",
    "def withLog(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with open(\"./etl_project_log.txt\", \"a\") as f:\n",
    "            f.write(datetime.datetime.now().strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__} start\\n\")\n",
    "            startTime = datetime.datetime.now()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            endTime = datetime.datetime.now()\n",
    "            f.write(endTime.strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__} end. in {endTime-startTime}\\n\")\n",
    "        return result\n",
    "    return wrapper   \n",
    "\n",
    "# get the gdp data from wikipedia\n",
    "#@withLog\n",
    "def extract_IMF_GDP(url):\n",
    "    # get http response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check reponse\n",
    "    if response.status_code == 200:\n",
    "        GDPJson= response.text\n",
    "    else : \n",
    "        print(response.status_code)\n",
    "\n",
    "    return GDPJson\n",
    "\n",
    "# transform the html table data to pandas data frame and process the data\n",
    "#@withLog\n",
    "def transform_IMF_GDP(data):\n",
    "    # html table to pandas data frame\n",
    "    df = json.loads(data)\n",
    "    df = df['values']['NGDPD']\n",
    "    df = pd.DataFrame(df).transpose()\n",
    "    df = df.fillna(0.0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# merge  imfData with ISO_Countries format data\n",
    "def mergeCountries(imfData, ISO_Countries):\n",
    "    iso = ISO_Countries.drop(columns=['alpha-2', 'country-code', 'iso_3166-2', 'region-code', 'sub-region-code', 'intermediate-region-code'])\n",
    "    imfData = imfData.reset_index()\n",
    "    result = pd.merge(\n",
    "        left=iso,\n",
    "        right=imfData,\n",
    "        left_on='alpha-3',\n",
    "        right_on='index',\n",
    "        how='inner'\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def printOver100B(gdpData, year):\n",
    "    print(gdpData[gdpData[str(year)] >= 100.0]['name'])\n",
    "\n",
    "# prin top 5 countries' gdp data average, year is integer and selectable\n",
    "def printTop5(gdpData, year):\n",
    "    top5 = gdpData.groupby('sub-region').apply(lambda x: x.sort_values(str(year)).head(5))\n",
    "    top5 = top5.reset_index(level = 0, drop=True)\n",
    "    top5 = top5.groupby('sub-region')[str(year)].mean()\n",
    "    print(top5)\n",
    "\n",
    "# write the dataFrame to a json file\n",
    "#@withLog\n",
    "def load(dataFrame):\n",
    "    # extract dataFrame by json\n",
    "    dataFrame.transpose().to_json('Countries_by_GDP.json')        \n",
    "\n",
    "ISO_Countries = pd.read_csv('./ISO_3166_Countries.csv')\n",
    "url = 'https://www.imf.org/external/datamapper/api/v1/NGDPD'\n",
    "\n",
    "a = transform_IMF_GDP(extract_IMF_GDP(url))\n",
    "b = mergeCountries(a, ISO_Countries)\n",
    "\n",
    "printOver100B(b, 2024)\n",
    "printTop5(b, 2024)\n",
    "\n",
    "load(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc1146",
   "metadata": {},
   "source": [
    "# ETL Process with sql\n",
    "다음은 IMF 에서 받은 데이터를 SQL에 load하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "341a416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Algeria',), ('Angola',), ('Argentina',), ('Australia',), ('Austria',), ('Bangladesh',), ('Belgium',), ('Brazil',), ('Bulgaria',), ('Canada',), ('Chile',), ('China',), ('Colombia',), ('Czechia',), ('Denmark',), ('Dominican Republic',), ('Ecuador',), ('Egypt',), ('Ethiopia',), ('Finland',), ('France',), ('Germany',), ('Greece',), ('Guatemala',), ('Hong Kong',), ('Hungary',), ('India',), ('Indonesia',), ('Iran, Islamic Republic of',), ('Iraq',), ('Ireland',), ('Israel',), ('Italy',), ('Japan',), ('Kazakhstan',), ('Kenya',), ('Korea, Republic of',), ('Kuwait',), ('Malaysia',), ('Mexico',), ('Morocco',), ('Netherlands, Kingdom of the',), ('New Zealand',), ('Nigeria',), ('Norway',), ('Oman',), ('Pakistan',), ('Peru',), ('Philippines',), ('Poland',), ('Portugal',), ('Puerto Rico',), ('Qatar',), ('Romania',), ('Russian Federation',), ('Saudi Arabia',), ('Singapore',), ('Slovakia',), ('South Africa',), ('Spain',), ('Sweden',), ('Switzerland',), ('Taiwan, Province of China',), ('Thailand',), ('Türkiye',), ('Ukraine',), ('United Arab Emirates',), ('United Kingdom of Great Britain and Northern Ireland',), ('United States of America',), ('Uzbekistan',), ('Venezuela, Bolivarian Republic of',), ('Viet Nam',)]\n",
      "[('Northern America', 2, 1566.69), ('Western Europe', 5, 398.46), ('Eastern Asia', 5, 306.34), ('Northern Europe', 5, 186.65), ('Southern Europe', 5, 160.52), ('Latin America and the Caribbean', 5, 147.6), ('Australia and New Zealand', 2, 92.75), ('Southern Asia', 5, 69.53), ('Western Asia', 5, 67.04), ('South-eastern Asia', 5, 46.38), ('Sub-Saharan Africa', 5, 44.54), (None, 1, 42.28), ('Eastern Europe', 10, 40.89), ('Northern Africa', 5, 28.61), ('Melanesia', 4, 1.42), ('Polynesia', 3, 0.1), ('Micronesia', 5, 0.03), ('Central Asia', 5, None)]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "# data base settings\n",
    "database = \"World_Economies.db\"\n",
    "# function to send a query\n",
    "def sendQuery(sql):\n",
    "    try:\n",
    "        with sqlite3.connect(database) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql)\n",
    "            conn.commit()\n",
    "            return cursor.fetchall()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    "\n",
    "# decorator for logging\n",
    "# this decorator helps to log the ETL processes.\n",
    "# function name, its start and end time, and running time will be logged.\n",
    "def withLog(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with open(\"./etl_project_log.txt\", \"a\") as f:\n",
    "            f.write(datetime.datetime.now().strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__} start\\n\")\n",
    "            startTime = datetime.datetime.now()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            endTime = datetime.datetime.now()\n",
    "            f.write(endTime.strftime(\"%Y-%B-%d-%H-%M-%S, \"))\n",
    "            f.write(f\"{func.__name__} end. in {endTime-startTime}\\n\")\n",
    "        return result\n",
    "    return wrapper   \n",
    "\n",
    "# get the gdp data from wikipedia\n",
    "#@withLog\n",
    "def extract_IMF_GDP(url):\n",
    "    # get http response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check reponse\n",
    "    if response.status_code == 200:\n",
    "        GDPJson= response.text\n",
    "    else : \n",
    "        print(response.status_code)\n",
    "\n",
    "    return GDPJson\n",
    "\n",
    "# transform the html table data to pandas data frame and process the data\n",
    "#@withLog\n",
    "def transform_IMF_GDP(data):\n",
    "    # html table to pandas data frame\n",
    "    df = json.loads(data)\n",
    "    df = df['values']['NGDPD']\n",
    "    df = pd.DataFrame(df).transpose()\n",
    "    return df\n",
    "\n",
    "# write the dataFrame to a json file\n",
    "#@withLog\n",
    "def load(dataFrame:pd.DataFrame, tableName:str, index:bool = False, index_label:str = \"\"):\n",
    "    # extract dataFrame by jsontry:\n",
    "    try:\n",
    "        with sqlite3.connect(database) as conn:\n",
    "            # Add table name 'gdp_data' and if_exists parameter\n",
    "            dataFrame.to_sql(tableName, conn, if_exists='replace', index=index, index_label=\"Country\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    \n",
    "# main\n",
    "\n",
    "# import gdp data from imf\n",
    "url = 'https://www.imf.org/external/datamapper/api/v1/NGDPD'\n",
    "imf_gdp = transform_IMF_GDP(extract_IMF_GDP(url))\n",
    "load(imf_gdp, \"imf_gdp\", index=True, index_label=\"country\")\n",
    "\n",
    "# import iso countries' name data from saved csv file\n",
    "ISO_Countries = pd.read_csv('./ISO_3166_Countries.csv')\n",
    "load(ISO_Countries, \"iso_country_name\")\n",
    "\n",
    "# print countries whose GDP is over 100B\n",
    "sql = \"\"\"SELECT DISTINCT i.name\n",
    "         FROM imf_gdp g\n",
    "         JOIN iso_country_name i ON i.\"alpha-3\" = g.country\n",
    "         WHERE g.\"2024\" >= 100.0;\"\"\"\n",
    "print(sendQuery(sql))\n",
    "\n",
    "# print top 5 GDP mean of group by region\n",
    "sql =\"\"\"WITH ranked_countries AS (\n",
    "            SELECT \n",
    "                g.country,\n",
    "                i.\"sub-region\",\n",
    "                g.\"1980\" as gdp,\n",
    "                RANK() OVER (PARTITION BY i.\"sub-region\" ORDER BY g.\"1980\" DESC) as rank\n",
    "            FROM imf_gdp g\n",
    "            JOIN iso_country_name i ON i.\"alpha-3\" = g.country\n",
    "            WHERE g.\"1980\" IS NOT NULL\n",
    "        )\n",
    "        SELECT \n",
    "            \"sub-region\",\n",
    "            COUNT(country) as country_count,\n",
    "            ROUND(AVG(gdp), 2) as avg_gdp\n",
    "        FROM ranked_countries\n",
    "        WHERE rank <= 5\n",
    "        GROUP BY \"sub-region\"\n",
    "        ORDER BY avg_gdp DESC;\"\"\"\n",
    "print(sendQuery(sql))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
