{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2M5: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 모듈을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: wordcloud in /opt/anaconda3/lib/python3.12/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install matplotlib\n",
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이하는 Sentiment140 dataset을 다운로드하고 압축을 푸는 코드입니다. 이미 csv파일이 있다면 실행하지 말아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Documents/GitHub/DataEngineering/W2/M5\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 80.9M  100 80.9M    0     0  11.7M      0  0:00:06  0:00:06 --:--:-- 13.4M\n",
      "Archive:  sentiment140.zip\n",
      "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! curl -L -o ./sentiment140.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/kazanova/sentiment140\n",
    "! unzip sentiment140.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그램을 작동하는데 필요한 모듈을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 멀티 프로세싱을 위해 쓰레드 수를 설정하고 큐를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "EXTRACT_TASK_COUNT = 4\n",
    "TRANSFORM_TASK_COUNT = 8\n",
    "LOAD_TASK_COUNT = 4\n",
    "\n",
    "CSV_DIR = \"training.1600000.processed.noemoticon.csv\"\n",
    "BATCH_SIZE = 1000\n",
    "WHOLE_SIZE = 1600000\n",
    "\n",
    "extractedQueuePos = mp.Queue()\n",
    "extractedQueueNeg = mp.Queue()\n",
    "transformedQueue = mp.Queue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 다운로드 받은 csv파일을 읽는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(sem, donePos, doneNeg):\n",
    "    startLine = sem.get()\n",
    "    while startLine < WHOLE_SIZE:\n",
    "        data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\n",
    "                           encoding='ISO-8859-1',\n",
    "                           names=[\"target\", \"id\",\"date\", \"flag\", \"user\", \"text\" ],\n",
    "                           skiprows=startLine,\n",
    "                           chunksize=BATCH_SIZE\n",
    "                          )\n",
    "        if not data[data[\"target\"] == 0].empty:\n",
    "            doneNeg.put(data[data[\"target\"] == 0])\n",
    "        if not data[data[\"target\"] == 4].empty:\n",
    "            donePos.put(data[data[\"target\"] == 4])\n",
    "        sem.put(BATCH_SIZE * EXTRACT_TASK_COUNT + startLine)\n",
    "        startLine = sem.get()\n",
    "    sem.put(startLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 각 문장을 token화 하는 Transform 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformTokinize(df):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    gTokens = []\n",
    "    for sentence in tqdm.tqdm(df, desc=f\"{mp.current_process().name}: tokenizing\"):\n",
    "        target = sentence[\"target\"]\n",
    "        sentence = sentence[\"text\"]\n",
    "        tokens = [t for t in nltk.word_tokenize(sentence)]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        tokens = [t for t in tokens if len(t) >= 3]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tokens = [lmtzr.lemmatize(t, 'v') for t in tokens]\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        [gTokens.append(t) for t in tokens]\n",
    "    data = pd.Series(gTokens)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data count =  1600000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\n",
    "                    encoding='ISO-8859-1',\n",
    "                    names=[\"target\", \"id\",\"date\", \"flag\", \"user\", \"text\" ]\n",
    "                    )\n",
    "print(\"data count = \", data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
